"""
RSS è·å–ä¸è§£ææ¨¡å—
è´Ÿè´£ä¸‹è½½ RSS XML å¹¶è§£æå‡ºç›®æ ‡æ—¥æœŸçš„å†…å®¹
"""
import feedparser
import requests
from datetime import datetime, timezone, timedelta
from typing import Optional, Dict, List, Any
from dateutil import parser as date_parser
import re

from src.config import RSS_URL, RSS_TIMEOUT


class RSSFetcher:
    """RSS è·å–å™¨"""

    def __init__(self, rss_url: str = None):
        self.rss_url = rss_url or RSS_URL
        self.timeout = RSS_TIMEOUT
        self._feed_data = None

    def fetch(self) -> feedparser.FeedParserDict:
        """ä¸‹è½½å¹¶è§£æ RSS"""
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ RSS: {self.rss_url}")

        try:
            response = requests.get(
                self.rss_url,
                timeout=self.timeout,
                headers={
                    "User-Agent": "Mozilla/5.0 (compatible; AI-Daily/1.0)"
                }
            )
            response.raise_for_status()

            # ä½¿ç”¨ feedparser è§£æ
            feed = feedparser.parse(response.content)

            if feed.bozo:
                print(f"âš ï¸ RSS è§£æè­¦å‘Š: {feed.bozo_exception}")

            print(f"âœ… RSS ä¸‹è½½æˆåŠŸï¼Œå…± {len(feed.entries)} æ¡èµ„è®¯")
            self._feed_data = feed
            return feed

        except requests.RequestException as e:
            raise Exception(f"RSS ä¸‹è½½å¤±è´¥: {e}")
        except Exception as e:
            raise Exception(f"RSS è§£æå¤±è´¥: {e}")

    def get_all_entries(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ¡ç›®"""
        if not self._feed_data:
            self.fetch()
        return self._feed_data.entries

    def get_content_by_date(self, target_date: str, feed: feedparser.FeedParserDict = None) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®æ—¥æœŸè·å–èµ„è®¯å†…å®¹

        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼: YYYY-MM-DD
            feed: RSS æ•°æ®ï¼Œå¦‚æœä¸ºç©ºåˆ™é‡æ–°è·å–

        Returns:
            åŒ¹é…çš„æ¡ç›®ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å› None
        """
        if feed is None:
            feed = self.fetch()

        # è§£æç›®æ ‡æ—¥æœŸ
        try:
            target_dt = datetime.strptime(target_date, "%Y-%m-%d")
            target_dt = target_dt.replace(tzinfo=timezone.utc)
        except ValueError:
            raise ValueError(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {target_date}ï¼ŒæœŸæœ›æ ¼å¼: YYYY-MM-DD")

        print(f"ğŸ” æ­£åœ¨æŸ¥æ‰¾æ—¥æœŸ: {target_date}")

        # å°è¯•å¤šç§æ–¹å¼åŒ¹é…æ—¥æœŸ
        for entry in feed.entries:
            # æ–¹æ³•1: æ£€æŸ¥ pubDate
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                pub_dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                if self._is_same_day(pub_dt, target_dt):
                    return self._extract_entry_content(entry)

            # æ–¹æ³•2: ä» link ä¸­æå–æ—¥æœŸ (æ ¼å¼: .../issues/YY-MM-DD-slug/)
            if hasattr(entry, 'link'):
                date_from_link = self._extract_date_from_link(entry.link)
                if date_from_link and date_from_link == target_date:
                    return self._extract_entry_content(entry)

        print(f"âŒ æœªæ‰¾åˆ°æ—¥æœŸ {target_date} çš„èµ„è®¯")
        return None

    def get_all_content_by_date(self, target_date: str, feed: feedparser.FeedParserDict = None) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®æ—¥æœŸè·å–å½“å¤©æ‰€æœ‰èµ„è®¯å†…å®¹ï¼ˆåˆå¹¶ä¸ºä¸€æ¡ï¼‰

        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼: YYYY-MM-DD
            feed: RSS æ•°æ®ï¼Œå¦‚æœä¸ºç©ºåˆ™é‡æ–°è·å–

        Returns:
            åˆå¹¶åçš„å†…å®¹å­—å…¸ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å› None
        """
        if feed is None:
            feed = self.fetch()

        # è§£æç›®æ ‡æ—¥æœŸ
        try:
            target_dt = datetime.strptime(target_date, "%Y-%m-%d")
            target_dt = target_dt.replace(tzinfo=timezone.utc)
        except ValueError:
            raise ValueError(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {target_date}ï¼ŒæœŸæœ›æ ¼å¼: YYYY-MM-DD")

        print(f"ğŸ” æ­£åœ¨æŸ¥æ‰¾æ—¥æœŸ: {target_date}")

        matched_entries = []

        # æ”¶é›†æ‰€æœ‰åŒ¹é…çš„æ¡ç›®
        for entry in feed.entries:
            matched = False

            # æ–¹æ³•1: æ£€æŸ¥ pubDate
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                pub_dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                if self._is_same_day(pub_dt, target_dt):
                    matched = True

            # æ–¹æ³•2: ä» link ä¸­æå–æ—¥æœŸ
            if not matched and hasattr(entry, 'link'):
                date_from_link = self._extract_date_from_link(entry.link)
                if date_from_link and date_from_link == target_date:
                    matched = True

            if matched:
                matched_entries.append(self._extract_entry_content(entry))

        if not matched_entries:
            print(f"âŒ æœªæ‰¾åˆ°æ—¥æœŸ {target_date} çš„èµ„è®¯")
            return None

        print(f"   æ‰¾åˆ° {len(matched_entries)} æ¡èµ„è®¯")

        # åˆå¹¶æ‰€æœ‰æ¡ç›®ä¸ºä¸€æ¡
        combined_content = self._combine_entries(matched_entries, target_date)
        return combined_content

    def _combine_entries(self, entries: List[Dict[str, Any]], target_date: str) -> Dict[str, Any]:
        """å°†å¤šæ¡èµ„è®¯åˆå¹¶ä¸ºä¸€æ¡"""
        if len(entries) == 1:
            return entries[0]

        # åˆå¹¶å†…å®¹
        content_parts = []
        for i, entry in enumerate(entries, 1):
            title = entry.get('title', 'æ— æ ‡é¢˜')
            content = entry.get('content', entry.get('description', ''))
            link = entry.get('link', '')
            content_parts.append(f"## {i}. {title}\n\n{content}\n\né“¾æ¥: {link}\n\n---\n")

        combined_content = "\n".join(content_parts)

        return {
            "title": f"{target_date} AI èµ„è®¯æ±‡æ€» ({len(entries)} æ¡)",
            "link": entries[0].get('link', ''),
            "guid": f"combined-{target_date}",
            "description": f"åŒ…å« {len(entries)} æ¡ AI èµ„è®¯",
            "content": combined_content,
            "pubDate": entries[0].get('pubDate', '')
        }

    def get_recent_content(self, hours: int = 24, feed: feedparser.FeedParserDict = None) -> Optional[Dict[str, Any]]:
        """
        è·å–æœ€è¿‘ N å°æ—¶å†…çš„æ‰€æœ‰èµ„è®¯

        Args:
            hours: æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤24å°æ—¶
            feed: RSS æ•°æ®ï¼Œå¦‚æœä¸ºç©ºåˆ™é‡æ–°è·å–

        Returns:
            åˆå¹¶åçš„å†…å®¹å­—å…¸ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å› None
        """
        if feed is None:
            feed = self.fetch()

        now = datetime.now(timezone.utc)
        cutoff_time = now - timedelta(hours=hours)

        print(f"ğŸ” æ­£åœ¨æŸ¥æ‰¾æœ€è¿‘ {hours} å°æ—¶çš„èµ„è®¯...")
        print(f"   æ—¶é—´èŒƒå›´: {cutoff_time.strftime('%Y-%m-%d %H:%M')} ~ {now.strftime('%Y-%m-%d %H:%M')} (UTC)")

        matched_entries = []

        for entry in feed.entries:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                pub_dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                if pub_dt >= cutoff_time:
                    matched_entries.append(self._extract_entry_content(entry))

        if not matched_entries:
            print(f"âŒ æœ€è¿‘ {hours} å°æ—¶å†…æ²¡æœ‰èµ„è®¯")
            return None

        print(f"   æ‰¾åˆ° {len(matched_entries)} æ¡èµ„è®¯")

        # ä½¿ç”¨ä»Šå¤©çš„æ—¥æœŸä½œä¸ºæ ‡è¯†
        today = now.strftime("%Y-%m-%d")
        combined_content = self._combine_entries(matched_entries, today)
        return combined_content, today

    def _is_same_day(self, dt1: datetime, dt2: datetime) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªæ—¥æœŸæ˜¯å¦æ˜¯åŒä¸€å¤©"""
        return (dt1.year, dt1.month, dt1.day) == (dt2.year, dt2.month, dt2.day)

    def _extract_date_from_link(self, link: str) -> Optional[str]:
        """ä»é“¾æ¥ä¸­æå–æ—¥æœŸï¼Œæ ¼å¼: YY-MM-DD æˆ– YYYY-MM-DD"""
        # åŒ¹é… /issues/26-01-13- æˆ– /issues/2026-01-13- æ ¼å¼
        patterns = [
            r'/issues/(\d{2})-(\d{2})-(\d{2})-',  # YY-MM-DD
            r'/issues/(\d{4})-(\d{2})-(\d{2})-',  # YYYY-MM-DD
        ]

        for pattern in patterns:
            match = re.search(pattern, link)
            if match:
                year, month, day = match.groups()
                # å¦‚æœæ˜¯ä¸¤ä½å¹´ä»½ï¼Œè½¬æ¢ä¸ºå››ä½
                if len(year) == 2:
                    year = "20" + year
                return f"{year}-{month}-{day}"

        return None

    def _extract_entry_content(self, entry) -> Dict[str, Any]:
        """æå–æ¡ç›®å†…å®¹"""
        content = {
            "title": "",
            "link": "",
            "guid": "",
            "description": "",
            "content": "",
            "pubDate": ""
        }

        # æå–æ ‡é¢˜
        content["title"] = entry.get("title", "")

        # æå–é“¾æ¥
        content["link"] = entry.get("link", "")

        # æå– GUID
        content["guid"] = entry.get("id", entry.get("guid", content["link"]))

        # æå–æè¿°
        content["description"] = entry.get("description", "")

        # æå–å®Œæ•´å†…å®¹
        if hasattr(entry, 'content') and entry.content:
            content["content"] = entry.content[0].get('value', '')
        elif hasattr(entry, 'summary'):
            content["content"] = entry.summary
        else:
            content["content"] = content["description"]

        # æå–å‘å¸ƒæ—¥æœŸ
        if hasattr(entry, 'published'):
            content["pubDate"] = entry.published
        elif hasattr(entry, 'updated'):
            content["pubDate"] = entry.updated

        # æ¸…ç† HTML å®ä½“
        content["content"] = content["content"].replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')

        return content

    def get_latest_date(self, feed: feedparser.FeedParserDict = None) -> Optional[str]:
        """è·å–æœ€æ–°çš„èµ„è®¯æ—¥æœŸ"""
        if feed is None:
            feed = self.fetch()

        if not feed.entries:
            return None

        # è·å–ç¬¬ä¸€æ¡çš„æ—¥æœŸ
        entry = feed.entries[0]

        # å°è¯•ä» link ä¸­æå–
        if hasattr(entry, 'link'):
            date_from_link = self._extract_date_from_link(entry.link)
            if date_from_link:
                return date_from_link

        # å°è¯•ä» pubDate ä¸­æå–
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
            return dt.strftime("%Y-%m-%d")

        return None

    def get_date_range(self, feed: feedparser.FeedParserDict = None) -> tuple:
        """è·å– RSS ä¸­çš„æ—¥æœŸèŒƒå›´"""
        if feed is None:
            feed = self.fetch()

        if not feed.entries:
            return None, None

        dates = []
        for entry in feed.entries:
            if hasattr(entry, 'link'):
                date_from_link = self._extract_date_from_link(entry.link)
                if date_from_link:
                    dates.append(date_from_link)

        if not dates:
            return None, None

        return min(dates), max(dates)


def fetch_rss_content(target_date: str) -> Optional[Dict[str, Any]]:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–æŒ‡å®šæ—¥æœŸçš„ RSS å†…å®¹"""
    fetcher = RSSFetcher()
    feed = fetcher.fetch()
    return fetcher.get_content_by_date(target_date, feed)
